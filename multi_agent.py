#!/usr/bin/env python

#multi_agent.py
#
#by Joe Hahn
#jmh.datasciences@gmail.com
#10 February 2018
#
#helper functions for an experiment with Q-learning on system having multiple agents 

#imports
import numpy as np
import pandas as pd
import random
import copy
from collections import deque

#initialize the environment = dict containing all constants that describe the system
def initialize_environment(rn_seed, max_turns, N_buckets, N_agents):
    random.seed(rn_seed)
    actions = range(N_buckets)
    acts = ['move to ' + str(action) for action in actions]
    bucket_params = []
    #generate set of parameters that govern each bucket's reward
    for idx in range(N_buckets):
        r0 = np.random.uniform(-1.0, 8.0)
        drdt = np.random.uniform(-8.0, 12.0)/max_turns
        Pmin = max_turns/50.0
        Pmax = max_turns/5.0
        P = np.random.uniform(Pmin, Pmax)
        omega = 2.0*np.pi/P
        phi = np.random.uniform(0, 2.0*np.pi)
        sinusoid_amp = 0*np.random.uniform(0.3, 3.0)
        random_amp = 0*10.0**np.random.uniform(-0.5, 1.5)
        bucket_params += [(r0, drdt, sinusoid_amp, omega, phi, random_amp)]
    environment = {'rn_seed':rn_seed, 'max_turns':max_turns, 'actions':actions,
        'acts':acts, 'N_buckets':N_buckets, 'N_agents':N_agents,
        'bucket_params':bucket_params}
    return environment

#initial state
def initialize_state(environment, turn):
    N_buckets = environment['N_buckets']
    N_agents = environment['N_agents']
    bucket_params = environment['bucket_params']
    all_buckets = range(N_buckets)
    agent_locations = np.random.choice(all_buckets, size=N_agents, replace=False)
    agent_locations.sort()
    state = {'agent_locations':agent_locations}
    state['next_agent'] = np.random.randint(0, N_agents)
    state['bucket_rewards'] = []
    update_bucket_rewards(state, environment, turn)
    return state

#increment time and update bucket rewards
def update_bucket_rewards(state, environment, turn):
    bucket_rewards = []
    for b_params in environment['bucket_params']:
        reward = get_bucket_reward(b_params, turn)
        bucket_rewards += [reward]
    state['bucket_rewards'] = bucket_rewards
    return

#reward generated by a bucket
def get_bucket_reward(b_params, turn):
    r0, drdt, sinusoid_amp, omega, phi, random_amp = b_params
    random_number = np.random.uniform(0.0, 1.0)
    reward = r0 + drdt*turn + sinusoid_amp*np.sin(omega*turn + phi) + random_amp*random_number
    return reward

#reward = sum of rewards generated by all occupied buckets
def get_reward(state):
    reward = 0.0
    bucket_rewards = state['bucket_rewards']
    for agent_location in state['agent_locations']:
        reward += bucket_rewards[agent_location]
    return reward

#convert state into a numpy array agent locations
def state2vector(state, environment):
    N_buckets = environment['N_buckets']
    v = np.zeros((1,N_buckets), dtype=float)
    for agent_location in state['agent_locations']:
        v[0, agent_location] += 1.0
    return v

#move agent
def move_agent(state, environment, action):
    state_next = copy.deepcopy(state)
    agent_locations = state_next['agent_locations']
    agent_idx = state_next['next_agent']
    agent_locations[agent_idx] = action
    N_agents = environment['N_agents']
    agent_idx += 1
    if (agent_idx >= N_agents):
        agent_idx = 0
    state_next['next_agent'] = agent_idx
    return state_next

#check game state = running, or too many moves
def get_game_state(turn, environment):
    game_state = 'running'
    max_turns = environment['max_turns']
    if (turn >= max_turns):
        game_state = 'max_turns'
    return game_state

#play one game using indicated strategy
def play_one_game(environment, turn, strategy, model=None):
    state = initialize_state(environment, turn)
    max_turns = environment['max_turns']
    N_buckets = environment['N_buckets']
    N_agents = environment['N_agents']
    memories_list = []
    game_state = get_game_state(turn, environment)
    while (game_state == 'running'):
        #move each agent
        for idx in range(N_agents):
            if (strategy == 'random'):
                action = np.random.choice(environment['actions'])
            if (strategy == 'smart'):
                state_vector = state2vector(state, environment)
                Q = model.predict(state_vector, batch_size=1)
                action = np.argmax(Q)
            state_next = move_agent(state, environment, action)
            reward = get_reward(state_next)
            memory = (turn, state, action, state_next, reward, game_state)
            memories_list += [memory]
            state = copy.deepcopy(state_next)
        #update bucket rewards after all agents have moved
        turn += 1
        update_bucket_rewards(state, environment, turn)
        game_state = get_game_state(turn, environment)
    N_memories = len(memories_list)
    memories = deque(maxlen=N_memories)
    for memory in memories_list:
        memories.append(memory)
    return memories   

#convert memories queue into timeseries dataframe
def memories2timeseries(memories):
    turns = []
    actions = []
    rewards = []
    bucket_rewards_list = []
    for memory in memories:
        turn, state, action, state_next, reward, game_state = memory
        turns += [turn]
        actions += [action]
        rewards += [reward]
        bucket_rewards = state['bucket_rewards']
        bucket_rewards_dict = {'bucket'+str(j):bucket_rewards[j] for j in range(len(bucket_rewards))}
        bucket_rewards_list += [bucket_rewards_dict]
    d = {'turn':turns, 'action':actions, 'reward':rewards, 'bucket_rewards':bucket_rewards_list}
    df = pd.DataFrame(d)
    cols = ['turn', 'action', 'reward']
    timeseries = pd.concat([df[cols], df['bucket_rewards'].apply(pd.Series)], axis=1)
    return timeseries

#generate memories of playing multiple random games
def play_N_games(environment, strategy, N_games):
    memories_list = []
    turn = 0
    N_memories = 0
    for N_game in range(N_games):
        memories = play_one_game(environment, turn, strategy)
        memories_list += [memories]
        N_memories += len(memories)
    memories = deque(maxlen=N_memories)
    for game_memories in memories_list:
        for m in game_memories:
            memories.append(m)
    return memories

#build neural network
def build_model(N_inputs, N_neurons, N_outputs):
    from keras.models import Sequential
    from keras.layers.core import Dense, Activation
    from keras.optimizers import RMSprop
    model = Sequential()
    model.add(Dense(N_neurons, input_shape=(N_inputs,)))
    model.add(Activation('relu'))
    model.add(Dense(N_neurons))
    model.add(Activation('relu'))
    model.add(Dense(N_outputs))
    model.add(Activation('linear'))
    rms = RMSprop()
    model.compile(loss='mse', optimizer=rms)
    return model

#train model via Q-learning
def train(environment, model, N_games, gamma, memories, batch_size, debug=False):
    epsilon = 1.0
    rewards = []
    epsilons = []
    games = range(N_games)
    for N_game in games:
        turn = 0
        state = initialize_state(environment, turn)
        state_vector = state2vector(state, environment)
        N_inputs = state_vector.shape[1]
        experience_replay = True
        turn = 0
        #ramp epsilon down
        if (epsilon > 0.1):
            epsilon -= 1.0/(0.4*N_games)
        N_agents = environment['N_agents']
        game_state = get_game_state(turn, environment)
        while (game_state == 'running'):
            #move each agent
            for idx in range(N_agents):
                state_vector = state2vector(state, environment)
                #predict this turn's possible rewards Q
                Q = model.predict(state_vector, batch_size=1)
                #choose best action
                if (np.random.random() < epsilon):
                    #choose random action
                    action = np.random.choice(environment['actions'])
                else:
                    #choose best action
                    action = np.argmax(Q)
                #get next state
                state_next = move_agent(state, environment, action)
                state_vector_next = state2vector(state_next, environment)
                #predict next turn's possible rewards
                Q_next = model.predict(state_vector_next, batch_size=1)
                max_Q_next = np.max(Q_next)
                reward = get_reward(state_next)
                #add next turn's discounted reward to this turn's predicted reward
                Q[0, action] = reward + gamma*max_Q_next
                #add to memory queue
                memory = (turn, state, action, state_next, reward, game_state)
                memories.append(memory)
                #update state
                state = copy.deepcopy(state_next)
            #experience replay trains model on randomly selected past experiences
            memories_sub = random.sample(memories, batch_size)
            turnz = [m[0] for m in memories_sub]
            statez = [m[1] for m in memories_sub]
            actionz = [m[2] for m in memories_sub]
            statez_next = [m[3] for m in memories_sub]
            rewardz = [m[4] for m in memories_sub]
            game_statez = [m[5] for m in memories_sub]
            state_vectorz_list = [state2vector(s, environment) for s in statez]
            state_vectorz = np.array(state_vectorz_list).reshape(batch_size, N_inputs)
            Qz = model.predict(state_vectorz, batch_size=batch_size)
            state_vectorz_next_list = [state2vector(s, environment) for s in statez_next]
            state_vectorz_next = np.array(state_vectorz_next_list).reshape(batch_size, N_inputs)
            Qz_next = model.predict(state_vectorz_next, batch_size=batch_size)
            for idx in range(batch_size):
                the_reward = rewardz[idx]
                max_Q_next = np.max(Qz_next[idx])
                the_action = actionz[idx]
                Qz[idx, the_action] = the_reward
                if (game_statez[idx] == 'running'):
                    Qz[idx, the_action] += gamma*max_Q_next
            model.fit(state_vectorz, Qz, batch_size=batch_size, epochs=1, verbose=0)
            #update bucket rewards after all agents have moved
            turn += 1
            update_bucket_rewards(state, environment, turn)
            game_state = get_game_state(turn, environment)
        #print something when game ends
        if (debug):
            print '======================='
            print 'game number = ', N_game
            print 'turn = ', turn
            print 'agent_locations = ', state_next['agent_locations']
            print 'reward = ', reward
            print 'epsilon = ', epsilon
            print 'game_state = ', game_state
        else:
            print '.',
        rewards += [reward]
        epsilons += [epsilon]
    return model, np.array(games), np.array(rewards), np.array(epsilons)
