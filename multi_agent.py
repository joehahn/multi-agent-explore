#!/usr/bin/env python

#multi_agent.py
#
#by Joe Hahn
#jmh.datasciences@gmail.com
#10 February 2018
#
#helper functions for an experiment with Q-learning on system having multiple agents 

#imports
import numpy as np
import pandas as pd
import random
import copy
from collections import deque

#initialize the environment = dict containing all constants that describe the system
def initialize_environment(rn_seed, max_turns, N_buckets, N_agents):
    random.seed(rn_seed)
    p0 = np.linspace(0.0, 0.01, num=N_buckets)
    sigma = np.sqrt(p0)
    #torpedo one-eighth of middle buckets with negative values of p0
    one_third = N_buckets/3
    p0[one_third : one_third + one_third/8] *= -1.0
    #torpedo one-fourth of the high-end buckets
    p0[-one_third/4:] *= -1.35
    bucket_params = {'p0':p0, 'sigma':sigma}
    environment = {'rn_seed':rn_seed, 'max_turns':max_turns, 'N_buckets':N_buckets,
        'N_agents':N_agents, 'bucket_params':bucket_params}
    return environment

#initial state
def initialize_state(environment):
    N_buckets = environment['N_buckets']
    N_agents = environment['N_agents']
    bucket_params = environment['bucket_params']
    all_locations = range(N_buckets)
    agent_locations = np.random.choice(all_locations, size=N_agents, replace=False)
    action2locations = pd.DataFrame()
    state = {'agent_locations':agent_locations}
    state['agent_value'] = np.ones(N_agents)
    state['bucket_productivity'] = bucket_productivity(environment)
    state['previous_bucket_productivity'] = bucket_productivity(environment)
    return state

#increment time and update bucket rewards
def bucket_productivity(environment):
    p0 = environment['bucket_params']['p0']
    sigma = environment['bucket_params']['sigma']
    bucket_productivity = np.random.normal(loc=p0, scale=sigma)
    idx = (bucket_productivity < -1.0)
    bucket_productivity[idx] = -1.0
    return bucket_productivity

#reward = sum of rewards generated by all occupied buckets
def get_reward(state):
    agent_locations = state['agent_locations']
    agent_value = state['agent_value']
    bucket_productivity = state['bucket_productivity']
    bucket_value = bucket_productivity[agent_locations]
    reward = (agent_value*(1.0 + bucket_value)).sum()
    return reward, bucket_value

#update bucket productivities and increment agent values
def update_state(state, bucket_value, environment):
    state_updated = copy.deepcopy(state)
    state_updated['previous_bucket_productivity'] = state_updated['bucket_productivity']
    state_updated['bucket_productivity'] = bucket_productivity(environment)
    agent_value = state_updated['agent_value']
    agent_value *= 1.0 + bucket_value
    state_updated['agent_value'] = agent_value
    return state_updated

#convert state into a numpy array of agent locations, agent values,
#and previous_bucket_productivity
def state2vector(state, environment):
    N_agents = environment['N_agents']
    N_buckets = environment['N_buckets']
    agent_locations = state['agent_locations']
    agent_value = state['agent_value']
    previous_bucket_productivity = state['previous_bucket_productivity']
    locations = np.zeros(N_buckets)
    for idx in range(N_agents):
        loc = agent_locations[idx]
        locations[loc] += 1.0
    v = np.concatenate([locations, agent_value, previous_bucket_productivity])
    return v.reshape(1, len(v))

#move agents
def move_agents(state, environment, strategy, locations=None):
    state_moved = copy.deepcopy(state)
    N_buckets = environment['N_buckets']
    N_agents = environment['N_agents']
    all_locations = range(N_buckets)
    one_third = N_buckets/3
    two_third = 2*N_buckets/3
    if (strategy == 'smart'):
        #use neural net to select best action
        action = 0
        locations = None
    if (strategy == 'random'):
        #agents move to buckets randomly selected
        action = 0
        allowed_locations = all_locations
        locations = np.random.choice(allowed_locations, size=N_agents, replace=True)
    if (strategy == 'low'):
        #agents move to buckets randomly selected from lower third
        action = 0
        allowed_locations = all_locations[0:one_third]
        locations = np.random.choice(allowed_locations, size=N_agents, replace=True)
    if (strategy == 'middle'):
        #agents move to buckets randomly selected from middle third
        action = 0
        allowed_locations = all_locations[one_third+1:two_third]
        locations = np.random.choice(allowed_locations, size=N_agents, replace=True)
    if (strategy == 'high'):
        #agents move to buckets randomly selected from upper third
        action = 0
        allowed_locations = all_locations[two_third+1:]
        locations = np.random.choice(allowed_locations, size=N_agents, replace=True)
    if (strategy == 'best'):
        #agents move to bucket having highest p0
        action = 0
        p0 = environment['bucket_params']['p0']
        allowed_locations = [p0.argmax()]
        locations = np.random.choice(allowed_locations, size=N_agents, replace=True)
    if (strategy == 'directed'):
        #agents move per locations optional parameter
        action = 0
    #update agent_locations
    state_moved['agent_locations'] = locations
    return state_moved, action

#check game state = running, or too many moves
def get_game_state(turn, environment):
    game_state = 'running'
    max_turns = environment['max_turns']
    if (turn >= max_turns):
        game_state = 'max_turns'
    return game_state

#play one game using indicated strategy
def play_one_game(environment, strategy, model=None):
    turn = 0
    N_agents = environment['N_agents']
    N_buckets = environment['N_buckets']
    memories_list = []
    state = initialize_state(environment)
    game_state = get_game_state(turn, environment)
    while (game_state == 'running'):
        #move agents per strategy
        state_moved, action = move_agents(state, environment, strategy)
        #get reward and bucket value
        reward, bucket_value = get_reward(state_moved)
        #update agents health, bucket_productivities, and action2locations
        state_next = update_state(state_moved, bucket_value, environment)
        memory = (turn, state, action, state_next, reward, game_state)
        memories_list += [memory]
        state = copy.deepcopy(state_next)
        turn += 1
        game_state = get_game_state(turn, environment)
    #generate memories queue
    N_memories = len(memories_list)
    memories = deque(maxlen=N_memories)
    for memory in memories_list:
        memories.append(memory)
    return memories 

#convert an array of integer locations to csv string
def locations_array2str(locations_array):
    loc = locations_array
    loc.sort()
    locations_str = str(loc.tolist()).strip('[').strip(']')
    return locations_str

#convert csv string of integer locations to array
def locations_str2array(locations_str): 
    locations_list = [int(loc) for loc in locations_str.split(',')]
    random.shuffle(locations_list)
    locations_array = np.array(locations_list)
    return locations_array

#convert memories queue into timeseries dataframe
def memories2timeseries(memories, environment):
    turns_list = []
    rewards_list = []
    agent_value_list = []
    locations_list = []
    N_buckets = environment['N_buckets']
    N_agents = environment['N_agents']
    for memory in memories:
        turn, state, action, state_next, reward, game_state = memory
        turns_list += [turn]
        rewards_list += [reward]
        agent_value = state_next['agent_value']
        agent_value_dict = {'agent_value_'+str(j):agent_value[j] for j in range(N_agents)}
        agent_value_list += [agent_value_dict]
        locations = state_next['agent_locations']
        locations_list += [locations_array2str(locations)]
    #generate reward_history dataframe
    d = {'turn':turns_list, 'reward':rewards_list, 'agent_value':agent_value_list}
    df = pd.DataFrame(d)
    df = pd.concat([df, df['agent_value'].apply(pd.Series)], axis=1)
    cols = ['turn', 'reward']
    cols += [col for col in df.columns if ('agent_value_' in col)]
    reward_history = df[cols]
    #generate location_rewards dataframe
    d = {'turn':turns_list, 'locations':locations_list, 'reward':rewards_list, 'N_visits':1}
    df = pd.DataFrame(d)
    df = df[['turn', 'locations', 'reward', 'N_visits']]
    N = df.groupby('locations', as_index=False).agg({'reward':sum, 'N_visits':sum})
    N['reward_per_agent'] = N['reward']*1.0/N['N_visits']/N_agents
    #Ns = N.sort_values('reward_per_agent', ascending=False).reset_index(drop=True)
    Ns = N.reset_index(drop=True)
    Ns['action'] = Ns.index
    cols = ['action', 'locations', 'reward', 'N_visits', 'reward_per_agent']
    location_rewards = Ns[cols]
    return reward_history, location_rewards

#generate memories of playing multiple random games
def play_N_games(environment, strategy, N_games):
    memories_list = []
    N_memories = 0
    for N_game in range(N_games):
        memories = play_one_game(environment, strategy)
        memories_list += [memories]
        N_memories += len(memories)
    memories = deque(maxlen=N_memories)
    for game_memories in memories_list:
        for m in game_memories:
            memories.append(m)
    return memories

#build an MLP neural network
def build_model(N_inputs, N_neurons, N_outputs):
    from keras.models import Sequential
    from keras.layers import Dense
    model = Sequential()
    model.add(Dense(N_neurons, activation='relu', input_shape=(N_inputs,)))
    model.add(Dense(N_neurons/2, activation='relu'))
    model.add(Dense(N_outputs, activation='linear'))
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model

#train model via Q-learning
def train(environment, model, N_games, epsilon, gamma, memories, actions_in, batch_size, debug=False):
    rewards = []
    N_agents = environment['N_agents']
    N_buckets = environment['N_buckets']
    actions = actions_in.copy()
    N_actions = len(actions)
    games = range(N_games)
    for N_game in games:
        turn = 0
        state = initialize_state(environment)
        state_vector = state2vector(state, environment)
        N_inputs = state_vector.shape[1]
        turn = 0
        game_state = get_game_state(turn, environment)
        while (game_state == 'running'):
            #choose action ie locations having highest future reward Q
            state_vector = state2vector(state, environment)
            Q = model.predict(state_vector, batch_size=1)
            action = np.argmax(Q)
            print Q
            print action
            idx = (actions['action'] == action)
            locations_str = actions['locations'][idx].values[0]
            locations = locations_str2array(locations_str)
            #move small number of agents to random locations
            random_numbers = np.random.uniform(0.0, 1.0, size=N_agents)
            random_locations = np.random.randint(0, N_buckets, size=N_agents)
            idx = (random_numbers < epsilon)
            locations[idx] = random_locations[idx]
            locations_str = locations_array2str(locations)
            #get next state
            strategy = 'directed'
            state_moved, action = move_agents(state, environment, strategy, locations=locations)
            reward, bucket_value = get_reward(state_moved)
            state_next = update_state(state_moved, bucket_value, environment)
            state_vector_next = state2vector(state_next, environment)
            #add to memory queue
            memory = (turn, state, action, state_next, reward, game_state)
            memories.append(memory)
            #update actions dataframe
            idx = (actions['locations'] == locations_str)
            if (idx.sum() > 0):
                actions.loc[idx, 'reward'] += reward
                actions.loc[idx, 'N_visits'] += 1
            else:
                reward_per_agent = reward/N_agents
                d = {'action':len(actions), 'location':locations_str, 'reward':reward, 
                    'N_visits':1, 'reward_per_agent':reward_per_agent}
                actions = actions.append(d, ignore_index=True).sort_values('reward_per_agent').reset_index(drop=True)
                actions['action'] = actions.index
                actions = actions[0:N_actions].copy()
            actions['reward_per_agent'] = actions['reward']*1.0/actions['N_visits']/N_agents
            #move agents and update turn & game_state
            state = copy.deepcopy(state_next)
            turn += 1
            game_state = get_game_state(turn, environment)
            #do experience replay ie train model on batch of randomly selected past experiences
            memories_sub = random.sample(memories, batch_size)
            turnz = [m[0] for m in memories_sub]
            statez = [m[1] for m in memories_sub]
            actionz = [m[2] for m in memories_sub]
            statez_next = [m[3] for m in memories_sub]
            rewardz = [m[4] for m in memories_sub]
            game_statez = [m[5] for m in memories_sub]
            state_vectorz_list = [state2vector(s, environment) for s in statez]
            state_vectorz = np.array(state_vectorz_list).reshape(batch_size, N_inputs)
            Qz = model.predict(state_vectorz, batch_size=batch_size)
            state_vectorz_next_list = [state2vector(s, environment) for s in statez_next]
            state_vectorz_next = np.array(state_vectorz_next_list).reshape(batch_size, N_inputs)
            Qz_next = model.predict(state_vectorz_next, batch_size=batch_size)
            for idx in range(batch_size):
                the_reward = rewardz[idx]
                max_Q_next = np.max(Qz_next[idx])
                the_action = actionz[idx]
                Qz[idx, the_action] = the_reward + gamma*max_Q_next
            model.fit(state_vectorz, Qz, batch_size=batch_size, nb_epoch=1, verbose=0)
        #print something when game ends
        if (debug):
            print '======================='
            print 'game number = ', N_game
            print 'turn = ', turn
            print 'action = ', action
            print 'agent_locations = ', state['agent_locations']
            print 'reward/N_agents = ', reward/N_agents
            print 'game_state = ', game_state
        else:
            print '.',
        rewards += [reward]
        rewards = np.array(rewards)
        games = np.array(games)
        actions = actions.sort_values('reward_per_agent', ascending=False).reset_index(drop=True)
    return model, games, rewards, actions
