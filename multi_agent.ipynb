{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi_agent.ipynb\n",
    "#\n",
    "#by Joe Hahn\n",
    "#jmh.datasciences@gmail.com\n",
    "#12 February 2018\n",
    "#\n",
    "#This uses Q-learning on multiple agents to demonstrate something..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game settings\n",
    "N_agents = 10               #number of agents\n",
    "N_buckets = 100             #number of buckets\n",
    "max_turns = 100             #max number of moves in single game\n",
    "sabotage_buckets = True     #sabaotage some buckets when true\n",
    "rn_seed = 15                #seed for random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import game\n",
    "from multi_agent import *\n",
    "import time\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import plotting libraries\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(font_scale=1.5, font='DejaVu Sans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize environment\n",
    "environment = initialize_environment(rn_seed, max_turns, N_buckets, N_agents, sabotage_buckets=sabotage_buckets)\n",
    "print 'environment = ', environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#initialize state\n",
    "state = initialize_state(environment)\n",
    "print 'state = ', state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move an agent\n",
    "agent = 1\n",
    "action = 2\n",
    "state_next = update_agents(state, agent, action, environment)\n",
    "print 'state_next = ', state_next\n",
    "reward = get_reward(state_next)\n",
    "print 'reward = ', reward\n",
    "turn = 0\n",
    "game_state = get_game_state(turn, environment)\n",
    "print 'game_state = ', game_state\n",
    "state_vector_next = state2vector(state_next, environment)\n",
    "print 'state_vector_next = ', state_vector_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot p0 and sigma:\n",
    "p0 = environment['bucket_params']['p0']\n",
    "sigma = environment['bucket_params']['sigma']\n",
    "xp = np.arange(len(p0))\n",
    "yp = p0\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 8))\n",
    "p = ax.set_title('bucket parameters')\n",
    "p = ax.set_xlabel('bucket')\n",
    "p = ax.set_ylabel('p0 and sigma')\n",
    "p = ax.plot(xp, yp, linewidth=1, marker='o', markersize=4, label='p0')\n",
    "yp = sigma\n",
    "p = ax.plot(xp, yp, linewidth=1, marker='o', markersize=4, label='sigma')\n",
    "p = ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#play one game of randomly-moving agents and stash history in dataframe\n",
    "strategy = 'random'\n",
    "memories = play_game(environment, strategy)\n",
    "reward_history = memories2timeseries(memories, environment)\n",
    "print 'number of memories = ', len(memories)\n",
    "reward_history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot per-agent reward versus turn, and agent values vs turn\n",
    "df = reward_history\n",
    "xp = df['turn']\n",
    "yp = df['reward']/N_agents\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 8))\n",
    "p = ax.set_title('per-agent reward vs turn')\n",
    "p = ax.set_xlabel('turn')\n",
    "p = ax.set_ylabel('reward per agent')\n",
    "p = ax.plot(xp, yp)\n",
    "#plot agents value versus turn\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 8))\n",
    "p = ax.set_title('agent value vs turn')\n",
    "p = ax.set_xlabel('turn')\n",
    "p = ax.set_ylabel('agent value')\n",
    "for col in df.columns:\n",
    "    if ('agent_value_' in col):\n",
    "        yp = df[col]\n",
    "        p = ax.plot(xp, df[col], label=col)\n",
    "p = ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#play a low-risk, low-reward game with all agents moving randomly among the lower third of buckets\n",
    "strategy = 'low'\n",
    "memories = play_game(environment, strategy)\n",
    "reward_history = memories2timeseries(memories, environment)\n",
    "df = reward_history\n",
    "xp = df['turn']\n",
    "yp = df['reward']/N_agents\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 8))\n",
    "p = ax.set_title('per-agent reward vs turn')\n",
    "p = ax.set_xlabel('turn')\n",
    "p = ax.set_ylabel('reward per agent')\n",
    "p = ax.plot(xp, yp)\n",
    "#plot agents value versus turn\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 8))\n",
    "p = ax.set_title('agent value vs turn')\n",
    "p = ax.set_xlabel('turn')\n",
    "p = ax.set_ylabel('agent value')\n",
    "for col in df.columns:\n",
    "    if ('agent_value_' in col):\n",
    "        yp = df[col]\n",
    "        p = ax.plot(xp, df[col], label=col)\n",
    "p = ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play a middle-risk, middle-reward game with all agents moving randomly among the middle third of buckets\n",
    "strategy = 'middle'\n",
    "memories = play_game(environment, strategy)\n",
    "reward_history = memories2timeseries(memories, environment)\n",
    "df = reward_history\n",
    "xp = df['turn']\n",
    "yp = df['reward']/N_agents\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 8))\n",
    "p = ax.set_title('per-agent reward vs turn')\n",
    "p = ax.set_xlabel('turn')\n",
    "p = ax.set_ylabel('reward per agent')\n",
    "p = ax.plot(xp, yp)\n",
    "#plot agents value versus turn\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 8))\n",
    "p = ax.set_title('agent value vs turn')\n",
    "p = ax.set_xlabel('turn')\n",
    "p = ax.set_ylabel('agent value')\n",
    "for col in df.columns:\n",
    "    if ('agent_value_' in col):\n",
    "        yp = df[col]\n",
    "        p = ax.plot(xp, df[col], label=col)\n",
    "p = ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play a high-risk, high-reward game with all agents moving randomly among the upper third of buckets\n",
    "strategy = 'high'\n",
    "memories = play_game(environment, strategy)\n",
    "reward_history = memories2timeseries(memories, environment)\n",
    "df = reward_history\n",
    "xp = df['turn']\n",
    "yp = df['reward']/N_agents\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 8))\n",
    "p = ax.set_title('per-agent reward vs turn')\n",
    "p = ax.set_xlabel('turn')\n",
    "p = ax.set_ylabel('reward per agent')\n",
    "p = ax.plot(xp, yp)\n",
    "#plot agents value versus turn\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 8))\n",
    "p = ax.set_title('agent value vs turn')\n",
    "p = ax.set_xlabel('turn')\n",
    "p = ax.set_ylabel('agent value')\n",
    "for col in df.columns:\n",
    "    if ('agent_value_' in col):\n",
    "        yp = df[col]\n",
    "        p = ax.plot(xp, df[col], label=col)\n",
    "p = ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play 20 games using various strategies, and generate stats for each strategy\n",
    "N_games = 20\n",
    "strategies = ['low', 'middle', 'random', 'high']#, 'best']\n",
    "reward_histories = pd.DataFrame()\n",
    "for strategy in strategies:\n",
    "    print 'strategy = ', strategy \n",
    "    for game in range(N_games):\n",
    "        memories = play_game(environment, strategy)\n",
    "        reward_history = memories2timeseries(memories, environment)\n",
    "        reward_history['strategy'] = strategy\n",
    "        reward_history['game'] = game\n",
    "        reward_histories = reward_histories.append(reward_history)\n",
    "game_stats = reward_histories.groupby(['strategy', 'turn'], as_index=False)['reward'].agg(['mean', 'std'])\n",
    "game_stats['std'] /= np.sqrt(N_games - 1)\n",
    "game_stats.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot average agent value vs turn, for each strategy\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 8))\n",
    "p = ax.set_title('per-agent reward vs strategy')\n",
    "p = ax.set_xlabel('turn')\n",
    "p = ax.set_ylabel('reward per agent')\n",
    "for strategy in strategies:\n",
    "    reward = game_stats['mean'][strategy]\n",
    "    std = game_stats['std'][strategy]\n",
    "    std /= N_agents\n",
    "    reward /= N_agents\n",
    "    xp = reward.index\n",
    "    yp = reward.values\n",
    "    err = std.values\n",
    "    p = ax.plot(xp, yp, label=strategy, markersize=4, marker='o')\n",
    "    ax.errorbar(xp, yp, yerr=std, alpha=0.4, color=p[0].get_color())\n",
    "p = ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play 1000 random games and store moves in memories queue\n",
    "N_games = 1000                  #set=0.1*N_training_games*batch_size\n",
    "strategy = 'random'\n",
    "memories_list = []\n",
    "N_memories = 0\n",
    "for N_game in range(N_games):\n",
    "    memories = play_game(environment, strategy)\n",
    "    memories_list += [memories]\n",
    "    N_memories += len(memories)\n",
    "memories = deque(maxlen=N_memories)\n",
    "for game_memories in memories_list:\n",
    "    for m in game_memories:\n",
    "        memories.append(m)\n",
    "print 'number of memories = ', len(memories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model\n",
    "state_vector = state2vector(state, environment)\n",
    "N_inputs = state_vector.shape[1]\n",
    "N_outputs = N_buckets\n",
    "N_neurons = 2*N_agents*N_buckets\n",
    "model = build_model(N_inputs, N_neurons, N_outputs)\n",
    "print 'N_agents = ', N_agents\n",
    "print 'N_buckets = ', N_buckets\n",
    "print 'N_inputs = ', N_inputs\n",
    "print 'N_neurons = ', N_neurons\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train model\n",
    "N_training_games = 500                     #number of games played during training\n",
    "gamma = 0.85                               #discount for future rewards\n",
    "batch_size = 20                            #number of memories used during experience-replay\n",
    "debug = False                              #set True to see stats about each game's final turn\n",
    "print 'batch_size = ', batch_size\n",
    "print '0.1*N_training_games*batch_size', 0.1*N_training_games*batch_size\n",
    "print 'training model'\n",
    "trained_model, game, cumulative_rewards, epsilons, final_action = \\\n",
    "    train(environment, model, N_training_games, gamma, memories, batch_size, debug=debug)\n",
    "print '\\ntraining done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot epsilon vs game_number\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 6))\n",
    "xp = game\n",
    "yp = epsilons\n",
    "p = ax.plot(xp, yp)\n",
    "p = ax.set_title('epsilon vs game number')\n",
    "p = ax.set_xlabel('game number')\n",
    "p = ax.set_ylabel('epsilon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot per-agent reward vs training game\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 6))\n",
    "xp = game\n",
    "yp = cumulative_rewards/N_agents/max_turns\n",
    "p = ax.plot(xp, yp)\n",
    "p = ax.set_title('reward vs training game')\n",
    "p = ax.set_xlabel('game')\n",
    "p = ax.set_ylabel('reward per agent-turn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot final_action vs game_number\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 6))\n",
    "xp = game\n",
    "yp = final_action\n",
    "p = ax.plot(xp, yp, marker='o', linestyle='-', markersize=5, alpha=0.5)\n",
    "p = ax.set_title('final_action vs game number')\n",
    "p = ax.set_xlabel('game number')\n",
    "p = ax.set_ylabel('final_action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#play smart game using trained model to select best action\n",
    "strategy = 'smart'\n",
    "memories = play_game(environment, strategy, model=model)\n",
    "reward_history = memories2timeseries(memories, environment)\n",
    "df = reward_history\n",
    "xp = df['turn']\n",
    "yp = df['reward']/N_agents\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 8))\n",
    "p = ax.set_title('per-agent reward vs turn')\n",
    "p = ax.set_xlabel('turn')\n",
    "p = ax.set_ylabel('reward per agent')\n",
    "p = ax.plot(xp, yp)\n",
    "#plot agents value versus turn\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 8))\n",
    "p = ax.set_title('agent value vs turn')\n",
    "p = ax.set_xlabel('turn')\n",
    "p = ax.set_ylabel('agent value')\n",
    "for col in df.columns:\n",
    "    if ('agent_value_' in col):\n",
    "        yp = df[col]\n",
    "        p = ax.plot(xp, df[col], label=col)\n",
    "p = ax.legend()\n",
    "#plot action vs turn\n",
    "xp = df['turn']\n",
    "yp = df['action']\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 8))\n",
    "p = ax.set_title('action vs turn')\n",
    "p = ax.set_xlabel('turn')\n",
    "p = ax.set_ylabel('action')\n",
    "p = ax.set_ylim(0, N_buckets)\n",
    "p = ax.plot(xp, yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play 10 games using various strategies and plot outcomes\n",
    "N_games = 10\n",
    "strategies = ['low', 'middle', 'high', 'smart', 'best']\n",
    "reward_histories = pd.DataFrame()\n",
    "for strategy in strategies:\n",
    "    print 'strategy = ', strategy \n",
    "    for game in range(N_games):\n",
    "        memories = play_game(environment, strategy, model=model)\n",
    "        reward_history = memories2timeseries(memories, environment)\n",
    "        reward_history['strategy'] = strategy\n",
    "        reward_history['game'] = game\n",
    "        reward_histories = reward_histories.append(reward_history)\n",
    "game_stats = reward_histories.groupby(['strategy', 'turn'], as_index=False)['reward'].agg(['mean', 'std'])\n",
    "game_stats['std'] /= np.sqrt(N_games - 1)\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 8))\n",
    "p = ax.set_title('per-agent reward vs strategy')\n",
    "p = ax.set_xlabel('turn')\n",
    "p = ax.set_ylabel('mean per-agent reward')\n",
    "for strategy in strategies:\n",
    "    reward = game_stats['mean'][strategy]\n",
    "    std = game_stats['std'][strategy]\n",
    "    std /= N_agents\n",
    "    reward /= N_agents\n",
    "    xp = reward.index\n",
    "    yp = reward.values\n",
    "    err = std.values\n",
    "    p = ax.plot(xp, yp, label=strategy, markersize=4, marker='o')\n",
    "    ax.errorbar(xp, yp, yerr=std, alpha=0.4, color=p[0].get_color())\n",
    "p = ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#done!\n",
    "time_stop = time.time()\n",
    "print 'execution time (minutes) = ', (time_stop - time_start)/60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
